{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science - CS 328 - Homework 1\n",
    "# Name and Instructions \n",
    "**Name**:Aryan Solanki <br>\n",
    "**Roll number**: 23110049 (BTech 2023-27)\n",
    "\n",
    "**Colaborators**: - Nupoor (23110224), Nishchay (23110222)\n",
    "\n",
    "### Details\n",
    "Your submission should be a single Jupyter notebook containing all the answers and the code. Use mark-\n",
    "down or LateX for the answers to the theoretical questions. You can, of course, work on Colab and submit\n",
    "the resulting notebook after downloading it.\n",
    "Copying code is not allowed, from others or any sources. Discussion with others is okay, but everything,\n",
    "both code and answers, has be be developed individually. Also give names of all collaborators.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "import struct\n",
    "from array import array\n",
    "from os.path  import join\n",
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 \n",
    "\n",
    "Suppose you define a clustering objective in the following manner â€“ give a partitioning \n",
    "$\\mathcal{C} = \\{C_1, \\dots, C_k\\}$, define\n",
    "\n",
    "$$\n",
    "\\text{cost}(\\mathcal{C}) = \\sum_i \\frac{1}{|C_i|} \\sum_{x,y \\in C_i} \\|x - y\\|_2^2\n",
    "$$\n",
    "\n",
    "i.e. cost a cluster is the sum of all pairwise squared distances. Give an algorithm for this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to show that the given cost function for \\( k \\)-means clustering  \n",
    "\n",
    "$$\n",
    "\\text{cost}(\\mathcal{C}) = \\sum_i \\frac{1}{|C_i|} \\sum_{x,y \\in C_i} \\|x - y\\|_2^2\n",
    "$$\n",
    "\n",
    "is equivalent to the standard \\( k \\)-means objective, which minimizes the sum of squared Euclidean distances from each point to its cluster mean:\n",
    "\n",
    "$$\n",
    "\\sum_i \\sum_{x \\in C_i} \\|x - \\mu_i\\|_2^2,\n",
    "$$\n",
    "\n",
    "where \\( \\mu_i \\) is the mean of cluster \\( C_i \\), defined as:\n",
    "\n",
    "$$\n",
    "\\mu_i = \\frac{1}{|C_i|} \\sum_{x \\in C_i} x.\n",
    "$$\n",
    "\n",
    "### Step-by-Step Proof:\n",
    "\n",
    "#### Step 1: Expand the Given Cost Function\n",
    "Expanding the squared Euclidean norm inside the sum:\n",
    "\n",
    "$$\n",
    "\\sum_{x,y \\in C_i} \\|x - y\\|_2^2 = \\sum_{x,y \\in C_i} \\left( \\|x\\|_2^2 + \\|y\\|_2^2 - 2 \\langle x, y \\rangle \\right).\n",
    "$$\n",
    "\n",
    "Using the trick of adding and subtracting $( \\mu )$:\n",
    "\n",
    "$$\n",
    "\\sum_{x,y \\in C_i} \\|x - y\\|_2^2 = \\sum_{x,y \\in C_i} \\| (x - \\mu) + (\\mu - y) \\|_2^2.\n",
    "$$\n",
    "\n",
    "Using the property \\( \\|a + b\\|_2^2 = \\|a\\|_2^2 + \\|b\\|_2^2 + 2 \\langle a, b \\rangle \\), we get:\n",
    "\n",
    "$$\n",
    "\\sum_{x,y \\in C_i} \\|x - y\\|_2^2 = \\sum_{x,y \\in C_i} \\left( \\|x - \\mu\\|_2^2 + \\|\\mu - y\\|_2^2 + 2 \\langle x - \\mu, \\mu - y \\rangle \\right).\n",
    "$$\n",
    "\n",
    "Now, splitting the sums:\n",
    "\n",
    "$$\n",
    "\\sum_{x,y \\in C_i} \\|x - y\\|_2^2 = \\sum_{x,y \\in C_i} \\|x - \\mu\\|_2^2 + \\sum_{x,y \\in C_i} \\|\\mu - y\\|_2^2 + 2 \\sum_{x,y \\in C_i} \\langle x - \\mu, \\mu - y \\rangle.\n",
    "$$\n",
    "\n",
    "Since the inner product term simplifies to zero due to the definition of the mean $( \\mu )$, we are left with:\n",
    "\n",
    "$$\n",
    "\\sum_{x,y \\in C_i} \\|x - y\\|_2^2 = \\sum_{x,y \\in C_i} \\|x - \\mu\\|_2^2 + \\sum_{x,y \\in C_i} \\|\\mu - y\\|_2^2.\n",
    "$$\n",
    "\n",
    "By symmetry of summation indices:\n",
    "\n",
    "$$\n",
    "\\sum_{x,y \\in C_i} \\|x - \\mu\\|_2^2 = \\sum_{x,y \\in C_i} \\|\\mu - y\\|_2^2.\n",
    "$$\n",
    "\n",
    "Thus, rewriting:\n",
    "\n",
    "$$\n",
    "\\sum_{x,y \\in C_i} \\|x - y\\|_2^2 = 2 \\sum_{x,y \\in C_i} \\|x - \\mu\\|_2^2.\n",
    "$$\n",
    "\n",
    "Now, dividing by $( |C_i| )$:\n",
    "\n",
    "$$\n",
    "\\frac{1}{|C_i|} \\sum_{x,y \\in C_i} \\|x - y\\|_2^2 = \\frac{2}{|C_i|} \\sum_{x,y \\in C_i} \\|x - \\mu\\|_2^2.\n",
    "$$\n",
    "\n",
    "Hence the standard \\( k \\)-means cost function is:\n",
    "\n",
    "$$\n",
    "\\sum_i \\sum_{x \\in C_i} \\|x - \\mu_i\\|_2^2,\n",
    "$$\n",
    "\n",
    "we conclude:\n",
    "\n",
    "$$\n",
    "\\text{cost}(\\mathcal{C}) = 2 \\times \\text{(standard k-means cost)}.\n",
    "$$\n",
    "\n",
    "Thus, the given cost function is equivalent to the standard \\( k \\)-means objective up to a factor of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important Definitions \n",
    "\n",
    "- `C` is a list with numpy arrays. \n",
    "- Each numpy array is 2d with some n x m. One numpy array represents a single cluster. \n",
    "- n rows represent, n points, and m represents the dimension of each of those points. \n",
    "- Hence, C[0] represents the first cluster. C[1] the second and so on. \n",
    "- `objective(C)` takes the current set of clusters, and returns the the `cost` and another list `centers`. \n",
    "- every element of `centers` is a m dimensionsal vector. This indicates the center of this given cluster combination. \n",
    "- `cost` is the cost calculated by the objective function. \n",
    "- `calculate_kmeans(C,num_iterations=5)` is the function that calculates the next cluster arrangement. It returns the final cluster as C. \n",
    "- let `calculate_kmeans(C, num_iterations=5)` use objective and print the cost for each cluster and also be able to get the centers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(C):\n",
    "    \"\"\"\n",
    "    Computes the K-means objective function.\n",
    "\n",
    "    Parameters:\n",
    "    - C: list of numpy arrays, each representing a cluster.\n",
    "\n",
    "    Returns:\n",
    "    - cost: float, the total cost of clustering.\n",
    "    - centers: list of numpy arrays, where each numpy array represents the cluster center.\n",
    "    \"\"\"\n",
    "    centers = []\n",
    "    total_cost = 0\n",
    "\n",
    "    for cluster in C: #iterating on o\n",
    "        if len(cluster) == 0:\n",
    "            #If cluster is empty, use a zero vector as its center\n",
    "            center = np.zeros(C[0].shape[1])\n",
    "        else:\n",
    "            #mean of each cluster/column\n",
    "            center = np.mean(cluster, axis=0)\n",
    "        \n",
    "        centers.append(center)\n",
    "\n",
    "        #distance from mean, summed\n",
    "        cost = 0\n",
    "        for x in cluster:\n",
    "            distance = np.linalg.norm(x - center) \n",
    "            cost += distance**2  \n",
    "        \n",
    "        total_cost += cost\n",
    "\n",
    "    return total_cost, centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters(X, centers):\n",
    "    \"\"\"\n",
    "    Assigns each data point to the nearest cluster center.\n",
    "\n",
    "    Parameters:\n",
    "    - X: numpy array of shape (N, m), where N is the number of points.\n",
    "    - centers: list of numpy arrays representing cluster centers.\n",
    "\n",
    "    Returns:\n",
    "    - C: list of numpy arrays, where each array contains the points in a cluster.\n",
    "    \"\"\"\n",
    "    k = len(centers)  \n",
    "    clusters = [[] for _ in range(k)]  #empty clusters\n",
    "\n",
    "    for x in X:\n",
    "        #point distance from centers\n",
    "        distances = []\n",
    "        for center in centers:\n",
    "            distance = np.linalg.norm(x - center)\n",
    "            distances.append(distance)\n",
    "\n",
    "        #index of closest center\n",
    "        closest_center = np.argmin(distances)\n",
    "        clusters[closest_center].append(x)\n",
    "\n",
    "    C = [np.array(cluster) for cluster in clusters]\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kmeans(X, k, num_iterations=5):\n",
    "    \"\"\"\n",
    "    Runs the K-means algorithm to find clusters.\n",
    "\n",
    "    Parameters:\n",
    "    - X: numpy array of shape (N, m), where N is the number of points.\n",
    "    - k: int, number of clusters.\n",
    "    - num_iterations: int, number of iterations.\n",
    "\n",
    "    Returns:\n",
    "    - C: final list of clusters.\n",
    "    \"\"\"\n",
    "    #intial center selection (random)\n",
    "    rng = np.random.default_rng(seed=2)\n",
    "    initial_indices = rng.choice(X.shape[0], size=k, replace=False)\n",
    "    centers = [X[i] for i in initial_indices]  # Initial cluster centers\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        #assignment\n",
    "        C = assign_clusters(X, centers)\n",
    "        #cost \n",
    "        cost, centers = objective(C)\n",
    "\n",
    "        print(f\"Iteration {i+1}, Cost: {cost}\")\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Sklearn vs our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "X = np.vstack([\n",
    "    np.random.normal(loc=[2, 2], scale=1.0, size=(50, 2)),   # Cluster 1\n",
    "    np.random.normal(loc=[8, 8], scale=1.0, size=(50, 2)),   # Cluster 2\n",
    "    np.random.normal(loc=[14, 14], scale=1.0, size=(50, 2))  # Cluster 3\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Cost: 2000.8783591490162\n",
      "Iteration 2, Cost: 1897.597396138707\n",
      "Iteration 3, Cost: 1889.9407980393114\n",
      "Iteration 4, Cost: 1889.5561321822352\n",
      "Iteration 5, Cost: 1889.5561321822352\n",
      "Iteration 6, Cost: 1889.5561321822352\n",
      "Iteration 7, Cost: 1889.5561321822352\n",
      "Iteration 8, Cost: 1889.5561321822352\n",
      "Iteration 9, Cost: 1889.5561321822352\n",
      "Iteration 10, Cost: 1889.5561321822352\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 3\n",
    "final_clusters = calculate_kmeans(X, k=num_clusters, num_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn K-Means Cost: [ 1.91507094 10.40537157 12.16720099  6.76651989  3.47553338  8.91803591\n",
      "  4.77395804  2.03045955  3.27809958  9.0766261   7.20372631  2.62417413\n",
      "  3.57289062  5.26763951  8.18159922  6.59318086  7.05645804  3.88953743\n",
      " 13.85224904  2.46614777  8.01877709 12.83802326 13.95216411  3.85283624\n",
      "  1.61033545  2.46981259  4.82688839  6.71068545  1.59220762 11.01074247\n",
      "  3.81250122  3.14398785  4.44817182  3.44975138 18.5581171   8.69250839\n",
      "  1.51239094  4.21992264  9.67329884  8.47317726  6.48048431  6.52074987\n",
      "  2.98265095  9.43392414  3.05887517  3.81425413 10.15141137  8.47697491\n",
      "  1.83117131  8.95855862]\n"
     ]
    }
   ],
   "source": [
    "#sklearn's KMeans\n",
    "kmeans_sklearn = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "kmeans_sklearn.fit(X)\n",
    "\n",
    "#cluster centers and cost\n",
    "sklearn_centers = kmeans_sklearn.cluster_centers_\n",
    "sklearn_labels = kmeans_sklearn.labels_\n",
    "\n",
    "#K-means objective for sklearn's clustering\n",
    "sklearn_cost = sum(np.linalg.norm(X[sklearn_labels == i] - center, axis=1)**2 for i, center in enumerate(sklearn_centers))\n",
    "print(f\"Sklearn K-Means Cost: {sklearn_cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of Cluster Centers:\n",
      "\n",
      "Self implemented Centers:\n",
      " [[ 0.68275573  2.07997942]\n",
      " [ 2.40220445  2.21195698]\n",
      " [11.02463784 11.01076082]]\n",
      "Sklearn implemented Centers:\n",
      " [[ 8.22657056  8.02991175]\n",
      " [13.73127887 13.88974164]\n",
      " [ 1.70218049  2.09033724]]\n"
     ]
    }
   ],
   "source": [
    "#our implementation\n",
    "_, our_centers = objective(final_clusters)\n",
    "\n",
    "print(\"\\nComparison of Cluster Centers:\\n\")\n",
    "print(\"Self implemented Centers:\\n\", np.array(our_centers))\n",
    "print(\"Sklearn implemented Centers:\\n\", sklearn_centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot based verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Plot original data\n",
    "# plt.scatter(X[:, 0], X[:, 1], c='gray', alpha=0.5, label=\"Data Points\")\n",
    "\n",
    "# # Plot our K-Means centers\n",
    "# our_centers_np = np.array(our_centers)\n",
    "# plt.scatter(our_centers_np[:, 0], our_centers_np[:, 1], c='red', marker='x', s=200, label=\"Self K-Means Centers\")\n",
    "\n",
    "# # Plot Sklearn's K-Means centers\n",
    "# plt.scatter(sklearn_centers[:, 0], sklearn_centers[:, 1], c='blue', marker='o', s=200, label=\"Sklearn Centers\")\n",
    "\n",
    "# plt.legend()\n",
    "# plt.title(\"Comparison of K-Means Implementations\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "For the k-means problem, show that there is at most a factor of four ratio between the optimal value when we either require all cluster centers to be data points or allow arbitrary points to be centers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution \n",
    "\n",
    "#### Cost with Arbitrary Cluster Centers\n",
    "When cluster centers can be **any point** in space, the optimal clustering cost is:\n",
    "\n",
    "$$\n",
    "\\text{cost}(\\mathbb{C}) = \\sum_{i=1}^{k} \\sum_{x \\in S_i} \\|x - c_i\\|_2^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $S_i$ is the set of points in the $i$-th cluster.\n",
    "- $c_i$ is the **centroid** (mean) of cluster $S_i$.\n",
    "- $k$ is the number of clusters.\n",
    "\n",
    "#### Cost with Cluster Centers Restricted to Data Points\n",
    "If we require that cluster centers must be **chosen from the data points**, the optimal clustering cost is:\n",
    "\n",
    "$$\n",
    "\\text{cost}(\\mathbb{C}_d) = \\sum_{i=1}^{k} \\sum_{x \\in S_i^d} \\|x - c_i^d\\|_2^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $S_i^d$ is the set of points in the $i$-th cluster under this constraint.\n",
    "- $c_i^d$ is the **data point closest to the centroid** of $S_i^d$.\n",
    "\n",
    "Our goal is to prove:\n",
    "\n",
    "$$\n",
    "\\text{cost}(\\mathbb{C}_d) \\leq 4 \\times \\text{cost}(\\mathbb{C})\n",
    "$$\n",
    "\n",
    "### Proof:\n",
    "\n",
    "#### Step 1: Using the Triangle Inequality\n",
    "By applying the **triangle inequality**, we get:\n",
    "\n",
    "$$\n",
    "\\|x - c_i^d\\|_2 \\leq \\|x - c_i\\|_2 + \\|c_i - c_i^d\\|_2\n",
    "$$\n",
    "\n",
    "#### Step 2: Squaring Both Sides\n",
    "Squaring both terms to convert the inequality to a form useful for sum-of-squares calculations:\n",
    "\n",
    "$$\n",
    "\\|x - c_i^d\\|_2^2 \\leq \\|x - c_i\\|_2^2 + \\|c_i - c_i^d\\|_2^2 + 2 \\cdot \\|x - c_i\\|_2 \\cdot \\|c_i - c_i^d\\|_2\n",
    "$$\n",
    "\n",
    "#### Step 3: Bounding $ \\|c_i - c_i^d\\|_2 $\n",
    "Since $c_i^d$ is the closest data point to $c_i$, we have:\n",
    "\n",
    "$$\n",
    "\\|c_i - x\\|_2 \\geq \\|c_i^d - c_i\\|_2 \\quad \\text{(for any } x \\text{ in cluster)}\n",
    "$$\n",
    "\n",
    "Substituting this bound into our inequality:\n",
    "\n",
    "$$\n",
    "\\|x - c_i^d\\|_2^2 \\leq \\|x - c_i\\|_2^2 + \\|c_i - x\\|_2^2 + 2 \\cdot \\|x - c_i\\|_2 \\cdot \\|c_i - x\\|_2\n",
    "$$\n",
    "\n",
    "Rearrange terms:\n",
    "\n",
    "$$\n",
    "\\|x - c_i^d\\|_2^2 \\leq 4 \\cdot \\|x - c_i\\|_2^2\n",
    "$$\n",
    "\n",
    "#### Step 4: Summing Over All Clusters\n",
    "Summing over all points in all clusters:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{k} \\sum_{x \\in S_i^d} \\|x - c_i^d\\|_2^2 \\leq \\sum_{i=1}^{k} \\sum_{x \\in S_i} 4 \\cdot \\|x - c_i\\|_2^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{cost}(\\mathbb{C}_d) \\leq 4 \\times \\text{cost}(\\mathbb{C})\n",
    "$$\n",
    "\n",
    "This proves that restricting cluster centers to data points increases the cost by at most a factor of **4**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Create a random variable X for which Markovâ€™s inequality is tight. Give proof for your answer. If it is\n",
    "tight, then why are we using other inequalities e.g. Chebyshev and Chernoff?\n",
    "\n",
    "### Solution \n",
    "\n",
    "Mathematically, Markovâ€™s inequality states:\n",
    "$ P(X \\geq t) \\leq \\frac{E[X]}{t}, \\quad \\text{for } X \\geq 0 \\text{ and } t > 0. $\n",
    "\n",
    "Or, can be remembered as, \"Markov's Inequality tells me how likely X will be atleast t\". \n",
    "\n",
    "It is **tight** when there exists some random variable $ X $ and threshold $ t $ such that:\n",
    "$ P(X \\geq t) = \\frac{E[X]}{t}. $\n",
    "\n",
    "For example, if $ X $ takes only two valuesâ€”0 with some probability and $ t $ with probability $ E[X]/t $â€”then the inequality holds with equality, meaning the bound is tight.\n",
    "\n",
    "Note:\n",
    "\n",
    "This is because Markov's inequality is most effective for distributions with:\n",
    "\n",
    "- One-sided support\n",
    "- Heavy right tails\n",
    "- Simple moment structures\n",
    "\n",
    "Hence, let \n",
    "\n",
    "$$\n",
    "P(X = x) =\n",
    "\\begin{cases} \n",
    "\\frac{1}{2}, & \\text{if } x = 0 \\\\ \n",
    "\\frac{1}{2}, & \\text{if } x = 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This represents a Bernoulli distribution with parameter \\( p = 0.5 \\):\n",
    "\n",
    "$$\n",
    "X \\sim \\text{Bernoulli}(p), \\quad \\text{where } p = 0.5\n",
    "$$\n",
    "\n",
    "So, \n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X] = \\frac{1}{2} \\times 0 + \\frac{1}{2} \\times 1 = \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "Hence, \n",
    "\n",
    "$$\n",
    "P(X \\geq t) \\leq \\frac{0.5}{t}\n",
    "$$ \n",
    "\n",
    "For \\( t = 1 \\), we can see:\n",
    "\n",
    "$$\n",
    "P(X = 1) \\leq \\frac{0.5}{1}\n",
    "$$\n",
    "\n",
    "Additionally, we want this bound to be **tight**. We observe that **Markov's inequality** achieves equality in this case, meaning we get a **tight bound**.\n",
    "\n",
    "#### Why do we need Chebyshevâ€™s and Chernoffâ€™s inequalities instead of just Markovâ€™s?\n",
    "\n",
    "Markovâ€™s inequality is too loose when we need tighter bounds on tail probabilities. It only uses the expectation and ignores variance or higher moments.\n",
    "\n",
    "Chebyshevâ€™s inequality helps Markovâ€™s by including variance, giving better control over deviations from the mean. Itâ€™s helps when we need concentrated results for distributions with finite variance. <br> \n",
    "\n",
    "Chernoff bounds go further by using moment generating functions(the MGF of X gives us all moments of X), which yield exponentially decreasing tail probabilities.<br> \n",
    "\n",
    "These are crucial in practical scenarios like ML, networking, and randomized algorithms, where tighter probability bounds help in risk management and performance guarantees.<br>\n",
    "\n",
    "#### Conclusion:\n",
    "Markovâ€™s inequality is useful but weakâ€”good for rough upper bounds. <br>\n",
    "\n",
    "Chebyshevâ€™s inequality is stronger when variance is known.<br>\n",
    "\n",
    "Chernoff bounds are best when we need exponentially small probabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "- Download the MNIST dataset from http://yann.lecun.com/exdb/mnist/. We will use the test dataset\n",
    "and test labels only. <br><br>\n",
    "(a) Cluster them first using k-means clustering, k = 10, with kmeans + + initialization (implement the\n",
    "complete Lloydâ€™s algorithm yourself). Check the Rand-index of the clustering against the true labels.\n",
    "Use the sklearn module for rand-index. <br><br>\n",
    "(b) Do the same for k-center clustering, k = 10. Implement the greedy algorithm discussed in class.\n",
    "Report the Rand-index here too. <br><br>\n",
    "(c) Run the single linkage agglomeration till there are k = 10 clusters. Report Rand-index here too. <br><br>\n",
    "(d) Run the same algorithms (k-means and k-center) but on a rank-k approximation of the training data\n",
    "matrix. Note that if A is the training data matrix (images Ã—pixels), then you can just use Uk Î£k for\n",
    "the clustering, no need to use Vk . Evaluate for k = 2, 5, 10 and report the rand-index values. <br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloaded from:\n",
    "https://www.kaggle.com/datasets/hojjatk/mnist-dataset?select=t10k-labels.idx1-ubyte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mnist test loading (Code taken from Kaggle)\n",
    "(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataloader(object):\n",
    "    def __init__(self, test_images_filepath, test_labels_filepath):\n",
    "        # self.training_images_filepath = training_images_filepath\n",
    "        # self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())        \n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())        \n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images[i][:] = img            \n",
    "        \n",
    "        return images, labels\n",
    "            \n",
    "    def load_data(self):\n",
    "        # x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (x_test, y_test)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x2000 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Set file paths based on added MNIST Datasets\n",
    "#\n",
    "input_path = './input'\n",
    "# training_images_filepath = join(input_path, 'train-images-idx3-ubyte/train-images-idx3-ubyte')\n",
    "# training_labels_filepath = join(input_path, 'train-labels-idx1-ubyte/train-labels-idx1-ubyte')\n",
    "test_images_filepath = join(input_path, 't10k-images.idx3-ubyte')\n",
    "test_labels_filepath = join(input_path, 't10k-labels.idx1-ubyte')\n",
    "# input/t10k-images.idx3-ubyte\n",
    "# /Users/aryan/Desktop/Academics /Semester 4/Data science/Assignment 1/input/t10k-labels.idx1-ubyte\n",
    "\n",
    "#\n",
    "# Helper function to show a list of images with their relating titles\n",
    "#\n",
    "def show_images(images, title_texts):\n",
    "    cols = 5\n",
    "    rows = int(len(images)/cols) + 1\n",
    "    plt.figure(figsize=(30,20))\n",
    "    index = 1    \n",
    "    for x in zip(images, title_texts):        \n",
    "        image = x[0]        \n",
    "        title_text = x[1]\n",
    "        plt.subplot(rows, cols, index)        \n",
    "        plt.imshow(image, cmap=plt.cm.gray)\n",
    "        if (title_text != ''):\n",
    "            plt.title(title_text, fontsize = 15);        \n",
    "        index += 1\n",
    "\n",
    "#\n",
    "# Load MINST dataset\n",
    "#\n",
    "mnist_dataloader = MnistDataloader(test_images_filepath, test_labels_filepath)\n",
    "(x_test, y_test) = mnist_dataloader.load_data()\n",
    "\n",
    "#\n",
    "# Show some random training and test images \n",
    "#\n",
    "images_2_show = []\n",
    "titles_2_show = []\n",
    "\n",
    "for i in range(0, 5):\n",
    "    r = random.randint(1, 10000)\n",
    "    images_2_show.append(x_test[r])        \n",
    "    titles_2_show.append('test image [' + str(r) + '] = ' + str(y_test[r]))    \n",
    "\n",
    "show_images(images_2_show, titles_2_show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000, 28, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test), len(y_test), len(x_test[0]), len(x_test[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensuring correct shape for MNIST Data\n",
    "x_test = np.array(x_test).reshape(len(x_test), -1)  #Flatten to (10000, 784)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import rand_score\n",
    "from scipy.spatial.distance import cdist\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansPP:\n",
    "    def __init__(self, clusters=10, max_iters=15, tol=1e-4):\n",
    "        self.clusters = clusters  \n",
    "        self.max_iters = max_iters  \n",
    "        self.tol = tol  #convergence threshold\n",
    "        self.cluster_labels = None  \n",
    "\n",
    "    def initialize_centroids(self, data):\n",
    "        #Selects centroid randomly\n",
    "        centroids = [data[np.random.choice(len(data))]]\n",
    "        \n",
    "        #Selects remaining centroids\n",
    "        for _ in range(1, self.clusters):\n",
    "            distances = np.min(cdist(data, centroids), axis=1)\n",
    "            distances /= distances.sum()\n",
    "            cum_prob = np.cumsum(distances)\n",
    "            \n",
    "            rand_val = np.random.rand()\n",
    "            for idx, value in enumerate(cum_prob):\n",
    "                if rand_val < value:\n",
    "                    centroids.append(data[idx])\n",
    "                    break\n",
    "        return np.array(centroids)\n",
    "\n",
    "    def cluster_data(self, data):\n",
    "        #Initializes centroids\n",
    "        self.centroids = self.initialize_centroids(data)\n",
    "        self.cluster_labels = np.zeros(len(data))\n",
    "        iteration = 0\n",
    "\n",
    "        while iteration < self.max_iters:\n",
    "            dist_matrix = cdist(data, self.centroids)\n",
    "            self.cluster_labels = np.argmin(dist_matrix, axis=1)\n",
    "            prev_centroids = self.centroids.copy()\n",
    "\n",
    "            for cluster_idx in range(self.clusters):\n",
    "                points_in_cluster = self.cluster_labels == cluster_idx\n",
    "                if np.sum(points_in_cluster) == 0:\n",
    "                    continue\n",
    "                self.centroids[cluster_idx] = np.mean(data[points_in_cluster], axis=0)\n",
    "\n",
    "            if np.linalg.norm(self.centroids - prev_centroids) < self.tol:\n",
    "                break\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "    def assign_clusters(self, data):\n",
    "        dist_matrix = cdist(data, self.centroids)\n",
    "        return np.argmin(dist_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rand index (K-Means++): 0.8914951095109511\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeansPP(clusters=10, max_iters=20)\n",
    "kmeans.cluster_data(x_test)\n",
    "\n",
    "predicted_labels = kmeans.assign_clusters(x_test)\n",
    "\n",
    "rand_index_kmeans = rand_score(y_test, predicted_labels)\n",
    "print(f\"Rand index (K-Means++): {rand_index_kmeans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KCenterClustering:\n",
    "    def __init__(self, num_clusters=10):\n",
    "        self.num_clusters = num_clusters  \n",
    "        self.cluster_assignments = None  \n",
    "\n",
    "    def cluster_data(self, dataset):\n",
    "        \"\"\"Performs K-Center clustering using NumPy only.\"\"\"\n",
    "        num_points, num_features = dataset.shape\n",
    "        self.centroids = np.zeros((self.num_clusters, num_features))\n",
    "        \n",
    "        self.centroids[0] = dataset[np.random.choice(num_points)]\n",
    "        \n",
    "        for i in range(1, self.num_clusters):\n",
    "            distances = np.min(np.linalg.norm(dataset[:, None] - self.centroids[:i], axis=2), axis=1)\n",
    "            \n",
    "            self.centroids[i] = dataset[np.argmax(distances)]\n",
    "        \n",
    "        self.cluster_assignments = np.argmin(np.linalg.norm(dataset[:, None] - self.centroids, axis=2), axis=1)\n",
    "\n",
    "    def assign_clusters(self, dataset):\n",
    "        \"\"\"Assigns clusters to new data points.\"\"\"\n",
    "        return np.argmin(np.linalg.norm(dataset[:, None] - self.centroids, axis=2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rand index (K-Center): 0.4895881188118812\n"
     ]
    }
   ],
   "source": [
    "kcenter = KCenterClustering(num_clusters=10)\n",
    "kcenter.cluster_data(x_test)\n",
    "\n",
    "predicted_labels_kcenter = kcenter.assign_clusters(x_test)\n",
    "\n",
    "rand_index_kcenter = rand_score(y_test, predicted_labels_kcenter)\n",
    "print(f\"Rand index (K-Center): {rand_index_kcenter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLinkageClustering:\n",
    "    def __init__(self, num_clusters=10):\n",
    "        self.num_clusters = num_clusters \n",
    "        self.cluster_assignments = []\n",
    "        self.N = None \n",
    "        self.cluster_ids = None\n",
    "        self.clusters = None\n",
    "\n",
    "    def cluster_data(self, X):\n",
    "        self.N = X.shape[0]\n",
    "        self.clusters = [[x] for x in X]\n",
    "        self.cluster_ids = [[i] for i in range(self.N)]\n",
    "        dist_matrix = np.full((self.N, self.N), np.inf)\n",
    "\n",
    "        while len(self.clusters) > self.num_clusters:\n",
    "        \n",
    "            if len(self.clusters) == self.N:\n",
    "                dist_matrix = np.linalg.norm(X[:, np.newaxis] - X, axis=2)\n",
    "        \n",
    "            id1, id2 = np.unravel_index(\n",
    "                np.argmin(dist_matrix), dist_matrix.shape)\n",
    "            cluster_id1, cluster_id2 = sorted([id1, id2])\n",
    "\n",
    "            for i in self.cluster_ids[cluster_id1]:\n",
    "                dist_matrix[i, self.cluster_ids[cluster_id2]] = np.inf\n",
    "\n",
    "            \n",
    "            self.clusters[cluster_id1].extend(self.clusters[cluster_id2])\n",
    "            self.clusters.pop(cluster_id2)\n",
    "\n",
    "            self.cluster_ids[cluster_id1].extend(self.cluster_ids[cluster_id2])\n",
    "            self.cluster_ids.pop(cluster_id2)\n",
    "\n",
    "        self.cluster_ids = self.cluster_ids\n",
    "        return self\n",
    "\n",
    "    def assign_clusters(self, X):\n",
    "        for x in X:\n",
    "            cluster_id = -1\n",
    "            for clust_index, cluster_ids in enumerate(self.cluster_ids):\n",
    "                if any(np.linalg.norm(X[p] - x) == 0 for p in cluster_ids):\n",
    "                    cluster_id = clust_index\n",
    "                    break\n",
    "            self.cluster_assignments.append(cluster_id)\n",
    "        return self.cluster_assignments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = x_test[:1000] #kernel was crashing, hence subset used\n",
    "subset_y = y_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rand index (Single Linkage): 0.11675675675675676\n"
     ]
    }
   ],
   "source": [
    "single_link = SingleLinkageClustering(num_clusters=10)\n",
    "single_link.cluster_data(subset)\n",
    "\n",
    "predicted_labels_single = single_link.assign_clusters(subset)\n",
    "\n",
    "rand_index_single = rand_score(subset_y, predicted_labels_single)\n",
    "print(f\"Rand index (Single Linkage): {rand_index_single}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kRankApprox:\n",
    "  def __init__(self,rank,k,model):\n",
    "    self.rank = rank \n",
    "    self.k = k \n",
    "    self.model = model \n",
    "    self.learner= None  \n",
    "  \n",
    "  def cluster_data(self,X):\n",
    "    u,sigma,v = np.linalg.svd(X,full_matrices=False)\n",
    "    u_rank = u[:, :self.rank]\n",
    "    sigma_rank = np.diag(sigma[:self.rank])\n",
    "    v_rank = v[:self.rank, :]\n",
    "    x_approx = np.dot(np.dot(u_rank,sigma_rank),v_rank)\n",
    "    \n",
    "    learner = self.model(self.k)\n",
    "    learner.cluster_data(x_approx)\n",
    "    self.learner = learner\n",
    "    return\n",
    "  \n",
    "  def get_rand_index(self,X, y):\n",
    "    labels = self.learner.assign_clusters(X)\n",
    "    return rand_score(y, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.copy(x_test)\n",
    "y_train = np.copy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>5</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>K-means</th>\n",
       "      <td>0.832987</td>\n",
       "      <td>0.870706</td>\n",
       "      <td>0.882214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K-centers</th>\n",
       "      <td>0.670434</td>\n",
       "      <td>0.800936</td>\n",
       "      <td>0.734799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 2         5         10\n",
       "K-means    0.832987  0.870706  0.882214\n",
       "K-centers  0.670434  0.800936  0.734799"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [KMeansPP, KCenterClustering]\n",
    "columss = [\"K-means\", \"K-centers\"]\n",
    "ranks = [2, 5, 10]\n",
    "\n",
    "results = []\n",
    "for i, model in enumerate(models):\n",
    "    col = columss[i]\n",
    "    res_col = []\n",
    "    for rank in ranks:\n",
    "        learner = kRankApprox(rank=rank, k=10, model=model)\n",
    "        learner.cluster_data(X_train)\n",
    "        res = learner.get_rand_index(X_train, y_train)\n",
    "        res_col.append(res)\n",
    "    results.append(res_col)\n",
    "\n",
    "pd.DataFrame(results, columns=ranks, index=columss) #for representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Suppose you have a population of 1 million people, out of which at least 1% are coffee drinkers. You want\n",
    "to get the estimate of this fraction by using sampling. Give the algorithm and the estimate. What kind\n",
    "of error bounds can you give with probability 99%?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution \n",
    "\n",
    "#### Algorithm \n",
    "\n",
    "The algorithm to estimate this fraction by using sampling is as follows: \n",
    "\n",
    "1. We assume that our initial dataset, N = 1 mil, is randomized. \n",
    "2. We will **uniformly and randomly** take a smaller subset out of this, say $n$\n",
    "\n",
    "    or each point has probability of choosing => \n",
    "\n",
    "    $$ 1/N $$\n",
    "\n",
    "    Here, ($n \\leq N$)\n",
    "        \n",
    "    (Note: The larger the n, the better the estimate).\n",
    "\n",
    "3. Now, out of n, we will check the labels 0 \"Not Coffee Drinker\" and 1 \"Coffee Drinker\", and compute the count for each of these.\n",
    "4. Then we will calculate the following ratios:\n",
    "\n",
    "    (a) $$label\\_zero\\_count / (n) $$\n",
    "    (b) $$label\\_one\\_count / (n) \\ or \\ X$$\n",
    "5. (b) can be said to be approximately 0.01. <br> (a) will be approximately 99.99.\n",
    "\n",
    "    (This is because of the estimate proven below.)\n",
    "\n",
    "#### Estimate\n",
    "\n",
    "We know that for N, $$p (or \\ number \\ of \\ coffee \\ drinkers) = 0.01$$ <br> 'p' indicates the fraction. Which for N turns out to be 10,000. \n",
    "\n",
    "Let, the estimate be, \n",
    "\n",
    "\n",
    "$$\\hat{p} = \\frac{X}{n}$$\n",
    "\n",
    "Here,  X  is the number of coffee drinkers in the sample of size  n.\n",
    "\n",
    "Now, we know that if the sampling is **uniform and random**, as we mentioned above, \n",
    "then, We can say,  $X \\sim \\text{Binomial}(n, p) $. \n",
    "\n",
    "This is because, the present problem can be modelled as a binomial selection problem. \n",
    "\n",
    "Hence, $\n",
    "\\mathbb{E}[\\hat{p}] = p\n",
    "$\n",
    "\n",
    "**More intuition:** <br>\n",
    "\n",
    "$X$ï¿¼ follows a Binomial distribution because we are conducting ï¿¼ independent Bernoulli trials, where each person in the sample has a fixed probability ï¿¼ of being a coffee drinker. If sampling is with replacement, each draw is independent, making ï¿¼. If sampling is without replacement, the exact distribution is Hypergeometric, but when ï¿¼, the Binomial approximation holds well. This is because the probability of selecting a coffee drinker remains nearly constant across draws. \n",
    "\n",
    "#### Error bounds that give 99% probability?\n",
    "\n",
    "Using Hoeffdingâ€™s inequality:  \n",
    "\n",
    "$$\n",
    "P(|\\hat{p} - p| \\geq \\epsilon) \\leq 2e^{-2n\\epsilon^2}\n",
    "$$\n",
    "\n",
    "For a **99% confidence bound**, we set:  \n",
    "\n",
    "$$\n",
    "2e^{-2n\\epsilon^2} \\leq 0.01\n",
    "$$\n",
    "\n",
    "This is because $P(|\\hat{p} - p| \\geq \\epsilon)$ indicated the probability of error being greater the $(\\epsilon * 100)\\%$. And we want the probability of error being less than 1% or we ant 99% confidence or $1- P(|\\hat{p} - p| \\geq \\epsilon) = 0.99$. Hence we make the equation such. \n",
    "\n",
    "\n",
    "Taking the natural logarithm:  \n",
    "\n",
    "$$\n",
    "-2n\\epsilon^2 \\leq \\ln(0.005)\n",
    "$$\n",
    "\n",
    "Approximating $ \\ln(0.005) \\approx -5.3 $, we get:  \n",
    "\n",
    "$$\n",
    "2n\\epsilon^2 \\geq 5.3\n",
    "$$\n",
    "\n",
    "For a **Â±1% error bound** $( \\epsilon = 0.01 )$:  \n",
    "\n",
    "$$\n",
    "2n(0.01)^2 \\geq 5.3\n",
    "$$\n",
    "\n",
    "Note: We are using  $( \\epsilon = 0.01 )$ as one case, we can use other error bounds as well. Example: **Â±1%, 10%, 15% etc. error bound**\n",
    "<br>\n",
    "$$\n",
    "2n(0.0001) \\geq 5.3\n",
    "$$\n",
    "\n",
    "$$\n",
    "n \\geq \\frac{5.3}{0.0002} = 26500\n",
    "$$\n",
    "\n",
    "Thus, a sample size of **$( n \\geq 26,500 )$** ensures that our estimate is within **Â±1% accuracy** with **99% confidence**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional\n",
    "\n",
    "The image (in Github repo) is from the book **Foundations of Data Science**, based on this we can use Hoeffding bound for this given situation to establish our error bound. \n",
    "\n",
    "In this case, we may even use **Chernoff Bound**. This choice depends on the situation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
